{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "Big Picture\n",
    "- What is vectorization?\n",
    "- When is it useful?\n",
    "- What can the compiler do?\n",
    "- How can you do it manually?\n",
    "\n",
    "\n",
    "All the code can be found at [https://github.com/wheatman/Vectorization_Notes](https://github.com/wheatman/Vectorization_Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization is a style of computer programming where operations are applied to multiple elements at the same time instead of individual elements.\n",
    "\n",
    "Vector code has special registers and special instructions.\n",
    "\n",
    "Most registers are between 8 and 64 bits\n",
    "\n",
    "![x86 registers](source/registers.JPG \"X86 registers\")\n",
    "\n",
    "Vector registers are between 128 and 512 bits currently.\n",
    "\n",
    "The vector instructions treat these bits as representing a vector of multiple elements of data and perform the operation element wise.\n",
    "\n",
    "For example, take a 128 bit register.\n",
    "\n",
    "We can think of these 128 bits as representing four 32-bit elements.\n",
    "\n",
    "We can then compare multiple registers each containing these four elements and get the results of all four comparisons at once\n",
    "\n",
    "\n",
    "![Vector Operation](source/vector_op.JPG \"Vector Operation\")\n",
    "\n",
    "\n",
    "The instructions I am describing here is [pcmpeqd](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpeq_epi32&ig_expand=878)\n",
    "\n",
    "Formally, given `a` and `b` each of which is a 128 bit register, output dest, a new 128 bit register calculated in the following way\n",
    "\n",
    "```\n",
    "FOR j := 0 to 3\n",
    "\ti := j*32\n",
    "\tdst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xFFFFFFFF : 0\n",
    "ENDFOR\n",
    "```\n",
    "\n",
    "If you want to operate on 256 bit registers you need to use a different instructions, such as [vpcmpeqd](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_cmpeq_epi32&ig_expand=878,879)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a problem.\n",
    "\n",
    "Given an array of data, we want to count how many ints are equal to the target.\n",
    "\n",
    "Here is some simple code to count.\n",
    "```c\n",
    "long count_ints(int *data, long n, int target) {\n",
    "  int count = 0;\n",
    "  for (int i = 0; i < n; i++) {\n",
    "    if (data[i] == target) {\n",
    "      count += 1;\n",
    "    }\n",
    "  }\n",
    "  return count;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us look at code and how it compiles we are going [Compiler Explorer](https://godbolt.org/)\n",
    "\n",
    "This website allows you to, among other things, compile code online and view the assembly.\n",
    "\n",
    "Lets start by looking at the assembly for this specific problem.\n",
    "\n",
    "We can see it [https://godbolt.org/z/8debMTGso](https://godbolt.org/z/8debMTGso)\n",
    "\n",
    "We can see that the compiler is able to do some vectorization for us\n",
    "\n",
    "\n",
    "Lets run these different versions and see the performance\n",
    "\n",
    "| Version | time to process 1 million bytes in microseconds |\n",
    "|--- | --- |\n",
    "| unoptimized | 172 |\n",
    "| optimized simple | 98 |\n",
    "| optimized | 90|\n",
    "| march=native | 45|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on compiler options\n",
    "One of the most important flags is `-O` which specifies the optimization level between `-O0` and `-O3`.  \n",
    "\n",
    "There are also special versions for optimizing for size `-Os` and `-Oz`.\n",
    "\n",
    "Lastly there is `-Ofast`  This can make code even faster, but at the price of correctness.  \n",
    "This mostly deals with floating point numbers, but it is something to be aware of.\n",
    "\n",
    "\n",
    "I am also sometimes using the flag `-fno-unroll-loops`  This disables loop unrolling and as such is normally not good for performance, but can help make the code smaller.  I use it here only to make the assembly a little easier to read.\n",
    "\n",
    "Then there is the `-m` flag.  This specifies which architecture to generate code for.  By default most compilers will generate code that will work on a very wide set of cpus, but you can use this flag to specify more specifically what system to run on.  This allows the compiler to generate code which is much faster on some cpus, but will crash other cpus.  \n",
    "\n",
    "`-mavx2` specifies a cpu which supports the avx2 extension\n",
    "\n",
    "`-march=native` says just generate code for whatever system I am running the compiler on. \n",
    "\n",
    "If you want to see what sort of extra hardware is on your machine you can look with `lscpu` or `cat /proc/cpuinfo`\n",
    "\n",
    "For example if we look at my laptop \n",
    "\n",
    "```\n",
    "Architecture:            x86_64\n",
    "  CPU op-mode(s):        32-bit, 64-bit\n",
    "  Address sizes:         46 bits physical, 48 bits virtual\n",
    "  Byte Order:            Little Endian\n",
    "CPU(s):                  20\n",
    "  On-line CPU(s) list:   0-19\n",
    "Vendor ID:               GenuineIntel\n",
    "  Model name:            13th Gen Intel(R) Core(TM) i9-13900H\n",
    "    CPU family:          6\n",
    "    Model:               186\n",
    "    Thread(s) per core:  2\n",
    "    Core(s) per socket:  10\n",
    "    Socket(s):           1\n",
    "    Stepping:            2\n",
    "    BogoMIPS:            5990.40\n",
    "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm serialize flush_l1d arch_capabilities\n",
    "Virtualization features: \n",
    "  Virtualization:        VT-x\n",
    "  Hypervisor vendor:     Microsoft\n",
    "  Virtualization type:   full\n",
    "Caches (sum of all):     \n",
    "  L1d:                   480 KiB (10 instances)\n",
    "  L1i:                   320 KiB (10 instances)\n",
    "  L2:                    12.5 MiB (10 instances)\n",
    "  L3:                    24 MiB (1 instance)\n",
    "```\n",
    "Specifically we are looking at the flags\n",
    "\n",
    "Here is a modern high end intel server\n",
    "\n",
    "```\n",
    "Flags:                           \n",
    "fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd ida arat avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear flush_l1d arch_capabilities\n",
    "```\n",
    "\n",
    "Now the flag names are a bit weird\n",
    "\n",
    " - SSE (Streaming SIMD Extensions)\n",
    " - AVX (Advanced Vector Extensions)\n",
    "\n",
    "\n",
    "SIMD stands for single instruction multiple data, which means we have a single instruction which processes multiple pieces of data at the same time\n",
    "AVX is basically just the successor to SIMD\n",
    "\n",
    "These both allow us to use vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try and understand how the vectorized code works\n",
    "\n",
    "```assembly\n",
    "count_ints(int*, long, int):                     # @count_ints(int*, long, int)\n",
    "        xor     eax, eax\n",
    "        test    rsi, rsi\n",
    "        jle     .LBB0_8\n",
    "        cmp     rsi, 8\n",
    "        jae     .LBB0_4\n",
    "        xor     ecx, ecx\n",
    "        jmp     .LBB0_3\n",
    ".LBB0_4:                                # %vector.ph\n",
    "        mov     rcx, rsi\n",
    "        and     rcx, -8\n",
    "        vmovd   xmm0, edx\n",
    "        vpbroadcastd    ymm1, xmm0\n",
    "        vpxor   xmm0, xmm0, xmm0\n",
    "        xor     eax, eax\n",
    ".LBB0_5:                                # %vector.body\n",
    "        vpcmpeqd        ymm2, ymm1, ymmword ptr [rdi + 4*rax]\n",
    "        vpsubd  ymm0, ymm0, ymm2\n",
    "        add     rax, 8\n",
    "        cmp     rcx, rax\n",
    "        jne     .LBB0_5\n",
    "        vextracti128    xmm1, ymm0, 1\n",
    "        vpaddd  xmm0, xmm0, xmm1\n",
    "        vpshufd xmm1, xmm0, 238                 # xmm1 = xmm0[2,3,2,3]\n",
    "        vpaddd  xmm0, xmm0, xmm1\n",
    "        vpshufd xmm1, xmm0, 85                  # xmm1 = xmm0[1,1,1,1]\n",
    "        vpaddd  xmm0, xmm0, xmm1\n",
    "        vmovd   eax, xmm0\n",
    "        cmp     rcx, rsi\n",
    "        je      .LBB0_7\n",
    ".LBB0_3:                                # %for.body\n",
    "        xor     r8d, r8d\n",
    "        cmp     dword ptr [rdi + 4*rcx], edx\n",
    "        sete    r8b\n",
    "        add     eax, r8d\n",
    "        inc     rcx\n",
    "        cmp     rsi, rcx\n",
    "        jne     .LBB0_3\n",
    ".LBB0_7:                                # %for.cond.cleanup.loopexit\n",
    "        mov     eax, eax\n",
    ".LBB0_8:                                # %for.cond.cleanup\n",
    "        vzeroupper\n",
    "        ret\n",
    "```\n",
    "\n",
    "```c++\n",
    "long count_ints(int * data, long n, int target) {\n",
    "    int count = 0;\n",
    "    int i = 0;\n",
    "    if (n > 8) {\n",
    "        int local_end = n - 8;\n",
    "        vector<int> vector_target = broadcast(target)\n",
    "        vector<int> vector_count = {0}\n",
    "        while (i < local_end) {\n",
    "            vector_count -= vector_equals(*(data+i), vector_target);\n",
    "            i+=8;\n",
    "        }\n",
    "        count += reduce_add(vector_count)\n",
    "    }\n",
    "    for (; i < n; i++) {\n",
    "        if (data[i] == target) {\n",
    "            count += 1;\n",
    "        }\n",
    "  }\n",
    "  return count;\n",
    "}\n",
    "\n",
    "long count_ints(int* data, long n, int target) {\n",
    "        int count = 0;\n",
    "        long clean_end = (n / 8) * 8;\n",
    "        long i = 0;\n",
    "        if (n > 8) {\n",
    "                int local_end = n - 8;\n",
    "                auto vector_target = _mm256_set1_epi32(target);\n",
    "                auto counts = _mm256_setzero_si256();\n",
    "                while (i < local_end) {\n",
    "                        auto vector_compare = _mm256_cmpeq_epi32(\n",
    "                                        _mm256_loadu_si256((__m256i *)(data + i)), vector_target)\n",
    "                        vector_count = _mm256_sub_epi32(counts, vector_compare);\n",
    "                        i+=8;\n",
    "                }\n",
    "                count += _mm256_extract_epi32(counts, 0);\n",
    "                count += _mm256_extract_epi32(counts, 1);\n",
    "                count += _mm256_extract_epi32(counts, 2);\n",
    "                count += _mm256_extract_epi32(counts, 3);\n",
    "                count += _mm256_extract_epi32(counts, 4);\n",
    "                count += _mm256_extract_epi32(counts, 5);\n",
    "                count += _mm256_extract_epi32(counts, 6);\n",
    "                count += _mm256_extract_epi32(counts, 7);\n",
    "        }=\n",
    "        for (; i < n; i++) {\n",
    "                if (data[i] == target) {\n",
    "                        count += 1;\n",
    "                }\n",
    "        }\n",
    "        return count;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a minor change to the program.\n",
    "\n",
    "```c++\n",
    "long count_ints(int* data, long n, int target) {\n",
    "    // change the type from an int to a long\n",
    "    long count = 0;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (data[i] == target) {\n",
    "            count+=1;\n",
    "        }\n",
    "    }\n",
    "    return count;\n",
    "}\n",
    "```\n",
    "We changed the type of the count variable from an int to a long\n",
    "\n",
    "This allows us to correctly count more than 2^31 elements\n",
    "\n",
    "We can see the new assembly [https://godbolt.org/z/sTdoK9oTr](https://godbolt.org/z/sTdoK9oTr)\n",
    "\n",
    "We can see how the non vectorized code does not change in any substantial way, but the vectorized code does change the inner loop fundamentally.\n",
    "\n",
    "\n",
    "\n",
    "The inner loop with an int count variable\n",
    "```\n",
    ".LBB0_5:                                # %vector.body\n",
    "        vpcmpeqd        ymm2, ymm1, ymmword ptr [rdi + 4*rax]\n",
    "        vpsubd  ymm0, ymm0, ymm2\n",
    "        add     rax, 8\n",
    "        cmp     rcx, rax\n",
    "        jne     .LBB0_5\n",
    "```\n",
    "\n",
    "The major issue is that the original approach was actually using the fact that two independant types were actually the same type.  That is to say the count, and the elements being compared.\n",
    "\n",
    "If the elements being compared are smaller than the count variable then you need to deal with overflow since the original approach uses these same bit positions to store the count distributed in a vector register.\n",
    "\n",
    "\n",
    "The inner loop with a long count variable \n",
    "\n",
    "\n",
    "```\n",
    "        vpcmpeqd        xmm3, xmm1, xmmword ptr [rdi + 4*rax]\n",
    "        vpmovzxdq       ymm3, xmm3              # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero\n",
    "        vpand   ymm3, ymm3, ymm2\n",
    "        vpaddq  ymm0, ymm0, ymm3\n",
    "        add     rax, 4\n",
    "        cmp     rcx, rax\n",
    "        jne     .LBB0_6\n",
    "```\n",
    "\n",
    "We see it here using `vpmovzxdq` which expands out 4 32 bit elements into 64 bit elements to then combine into 4 64 bit counts which are then large enough.\n",
    "\n",
    "While this approach makes sense for transforming ints to longs, does it still make sense if we are instead looking for bytes.  In that case we could compare up to 32 bytes at a time, but we will still be limited to only doing the 4 longs that fit into a 256 bit vector.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a second problem.\n",
    "\n",
    "Here we get an array of bytes, and we want to know how many times the target bytes appears next to itself.  \n",
    "\n",
    "```c\n",
    "uint64_t count_pairs(uint8_t *data, uint64_t size, uint8_t target) {\n",
    "  uint64_t total = 0;\n",
    "  uint16_t check = target | (target << 8U);\n",
    "  for (uint64_t i = 0; i < size * 2 - 1; i++) {\n",
    "    total += (load16(data + i) == check);\n",
    "  }\n",
    "  return total;\n",
    "}\n",
    "```\n",
    "\n",
    "This problem seems very similar to the one we looked at before\n",
    "\n",
    "However, the fact that we are looking at unaligned data makes it a fair amount messier.\n",
    "\n",
    "First let's take a look at the performance of this code\n",
    "\n",
    "| Version | time to process  2 million bytes in microseconds |\n",
    "|--- | --- |\n",
    "| unoptimized | 2957 |\n",
    "| optimized simple | 489 |\n",
    "| optimized | 481 |\n",
    "| march=native | 513 |\n",
    "\n",
    "We notice that we are not able to see much speedups with `-march=native`\n",
    "\n",
    "Lets take a look at the code\n",
    "\n",
    "[https://godbolt.org/z/8Yasv3vW1](https://godbolt.org/z/8Yasv3vW1)\n",
    "\n",
    "The vectorized loop is messy as it tries to rearrange  the data to fit into a standard pattern it knows how to vectorize.\n",
    "\n",
    "Now we have a few options.  The first option is to convert the code into something that is easier to vectorize.  Now we know it can handle standard shorts, so we can just run through the loop twice, once for each alignment.\n",
    "\n",
    "```c\n",
    "uint64_t __attribute__((noinline))\n",
    "count_pairs(uint8_t *data, uint64_t size, uint8_t target) {\n",
    "  uint64_t total = 0;\n",
    "  uint16_t check = target | (target << 8U);\n",
    "  for (uint64_t i = 0; i < size * 2 - 1; i+=2) {\n",
    "    if (load16(data + i) == check) {\n",
    "      total +=1;\n",
    "    }\n",
    "  }\n",
    "  for (uint64_t i = 1; i < size * 2 - 1; i+=2) {\n",
    "    if (load16(data + i) == check) {\n",
    "      total +=1;\n",
    "    }\n",
    "  }\n",
    "  return total;\n",
    "}\n",
    "```\n",
    "\n",
    "Now the compiler should be able to easily vectorize both loops individually\n",
    "We can take a look at the code generated [https://godbolt.org/z/9T97PPn46](https://godbolt.org/z/9T97PPn46)\n",
    "\n",
    "| Version | time to process  2 million bytes in microseconds |\n",
    "|--- | --- |\n",
    "| unoptimized | 2957 |\n",
    "| optimized simple | 489 |\n",
    "| optimized | 481 |\n",
    "| march-native | 513 |\n",
    "| split loops | 346 | \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsics\n",
    "\n",
    "The compiler can do a lot to optimize and use the vector registers to speed up the code, but we can also do it manually.\n",
    "\n",
    "Intrinsics are basically functions which will compile down to a single special assembly instruction that we can use to use the vector instructions in c or c++ code without having to write raw assembly.  The list of them can be found [https://software.intel.com/sites/landingpage/IntrinsicsGuide/](https://software.intel.com/sites/landingpage/IntrinsicsGuide/)\n",
    "\n",
    "![_mm256_cmpeq_epi16](source/_mm256_cmpeq_epi16.JPG \"_mm256_cmpeq_epi16\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can still do it by hand.\n",
    "\n",
    "```c++\n",
    "\n",
    "unsigned long count_pairs(unsigned char *data, unsigned long size, unsigned char target) {\n",
    "  unsigned long total = 0;\n",
    "  unsigned short check = target | (target << 8U);\n",
    "  __m256i compare = _mm256_set1_epi16(target);\n",
    "  for (uint64_t i = 0; i < size * 2; i += 32) {\n",
    "    uint32_t block1 = _mm256_movemask_epi8(\n",
    "        _mm256_cmpeq_epi16(_mm256_load_si256((__m256i *)(data + i)), compare));\n",
    "    uint32_t block2 = _mm256_movemask_epi8(_mm256_cmpeq_epi16(\n",
    "        _mm256_loadu_si256((__m256i *)(data + i + 1)), compare));\n",
    "    total += __builtin_popcount(block1);\n",
    "    total += __builtin_popcount(block2);\n",
    "  }\n",
    "  return total / 2;\n",
    "}\n",
    "```\n",
    "\n",
    "This approach does a similar thing to the idea above in splitting the loops to check the two alignments, but in this case we do both steps at the same time.\n",
    "\n",
    "Lets walk through this code together\n",
    "\n",
    "we use \n",
    "`_mm256_load_si256((__m256i *)(data + i))` and `_mm256_loadu_si256((__m256i *)(data + i + 1))` to load the two blocks of data, offset from each other by 1 bytes\n",
    "\n",
    "\n",
    "then we compare each of these to the vector with the comparison with `_mm256_cmpeq_epi16`  \n",
    "\n",
    "This, as described above, sets all of the bits if the corresponding positions are equal.\n",
    "\n",
    "Then we use `_mm256_movemask_epi8` to pull the result of the comparison from vector registers back to normal registers.  This instruction gives us the top bit from each byte.\n",
    "\n",
    "Lastly we just count how many bits are set in each of our comparisons with `__builtin_popcount`\n",
    "\n",
    "Now since we pulled out bytes, even though we compared by shorts we need to divide by 2 at the end.\n",
    "\n",
    "So altogether we get a performance of\n",
    "\n",
    "| Version | time to process  2 million bytes in microseconds |\n",
    "|--- | --- |\n",
    "| unoptimized | 2957 |\n",
    "| optimized simple | 489 |\n",
    "| optimized | 481 |\n",
    "| march-native | 513 |\n",
    "| split loops | 346 | \n",
    "| manual vectorization | 117 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "- vectorization can help us speed up our code\n",
    "- the compiler can help us with many forms of vectorization\n",
    "- when the compiler can't help us we can do it manually using intrinsics\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
